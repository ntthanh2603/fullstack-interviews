# ğŸ–¼ï¸ Convolutional Neural Network (CNN)

## ğŸ“– Giá»›i thiá»‡u

Convolutional Neural Network (CNN) lÃ  má»™t kiáº¿n trÃºc máº¡ng neural sÃ¢u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u cÃ³ cáº¥u trÃºc dáº¡ng lÆ°á»›i nhÆ° hÃ¬nh áº£nh. CNN Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn cÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a vá» nÃ£o thá»‹ giÃ¡c cá»§a Ä‘á»™ng váº­t, láº§n Ä‘áº§u Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Yann LeCun vÃ o nhá»¯ng nÄƒm 1980s vÃ  trá»Ÿ thÃ nh ná»n táº£ng cá»§a computer vision hiá»‡n Ä‘áº¡i.

![CNN](../assets/CNNs.jpg)

## ğŸ—ï¸ Kiáº¿n trÃºc

CNN bao gá»“m cÃ¡c thÃ nh pháº§n chÃ­nh:

### 1. Convolutional Layer (Lá»›p tÃ­ch cháº­p)

- Ãp dá»¥ng cÃ¡c bá»™ lá»c (filters/kernels) Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng
- Giá»¯ nguyÃªn tÃ­nh cháº¥t khÃ´ng gian cá»§a dá»¯ liá»‡u

### 2. Pooling Layer (Lá»›p gá»™p)

- Giáº£m kÃ­ch thÆ°á»›c dá»¯ liá»‡u (downsampling)
- TÄƒng tÃ­nh báº¥t biáº¿n vá»›i vá»‹ trÃ­ (translation invariance)

### 3. Fully Connected Layer (Lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§)

- Lá»›p MLP truyá»n thá»‘ng á»Ÿ cuá»‘i máº¡ng
- Thá»±c hiá»‡n phÃ¢n loáº¡i hoáº·c há»“i quy

### 4. Activation Functions (HÃ m kÃ­ch hoáº¡t)

- ReLU, Sigmoid, Tanh, Leaky ReLU

## âš™ï¸ CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng

### Convolution Operation

1. **Bá»™ lá»c**: Kernel trÆ°á»£t qua input vá»›i stride nháº¥t Ä‘á»‹nh
2. **Dot product**: TÃ­nh tÃ­ch vÃ´ hÆ°á»›ng táº¡i má»—i vá»‹ trÃ­
3. **Feature map**: Táº¡o ra báº£n Ä‘á»“ Ä‘áº·c trÆ°ng má»›i
4. **Padding**: ThÃªm padding Ä‘á»ƒ kiá»ƒm soÃ¡t kÃ­ch thÆ°á»›c output

### Pooling Operation

1. **Max Pooling**: Láº¥y giÃ¡ trá»‹ lá»›n nháº¥t trong vÃ¹ng
2. **Average Pooling**: TÃ­nh giÃ¡ trá»‹ trung bÃ¬nh
3. **Global Pooling**: Ãp dá»¥ng trÃªn toÃ n bá»™ feature map

### Forward Pass

1. Input â†’ Convolution â†’ Activation â†’ Pooling
2. Láº·p láº¡i qua nhiá»u lá»›p conv-pool
3. Flatten â†’ Fully Connected â†’ Output

## ğŸ§® CÃ´ng thá»©c

### Convolution

```
Y[i,j] = Î£â‚˜ Î£â‚™ X[i+m, j+n] Ã— K[m,n] + b
```

### Output Size

```
Output = (Input + 2Ã—Padding - Kernel) / Stride + 1
```

### Pooling

```
Max Pooling: Y[i,j] = max(X[iÃ—s:iÃ—s+k, jÃ—s:jÃ—s+k])
Avg Pooling: Y[i,j] = mean(X[iÃ—s:iÃ—s+k, jÃ—s:jÃ—s+k])
```

Trong Ä‘Ã³:

- `X`: Input feature map
- `K`: Kernel/Filter
- `Y`: Output feature map
- `s`: Stride
- `k`: Kernel size
- `b`: Bias

## ğŸ“Š CÃ¡c loáº¡i CNN phá»• biáº¿n

### LeNet (1998)

- Máº¡ng CNN Ä‘áº§u tiÃªn cho nháº­n dáº¡ng chá»¯ viáº¿t
- Conv â†’ Pool â†’ Conv â†’ Pool â†’ FC â†’ FC

### AlexNet (2012)

- Äá»™t phÃ¡ trong ImageNet competition
- Sá»­ dá»¥ng ReLU vÃ  Dropout

### VGGNet (2014)

- Sá»­ dá»¥ng kernel 3Ã—3 nhá», máº¡ng sÃ¢u
- VGG-16, VGG-19

### ResNet (2015)

- Giá»›i thiá»‡u Skip Connection
- Giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient

### Inception/GoogLeNet (2014)

- Multi-scale feature extraction
- Inception modules

## âœ… Æ¯u Ä‘iá»ƒm

- **Translation Invariance**: Báº¥t biáº¿n vá»›i vá»‹ trÃ­ Ä‘á»‘i tÆ°á»£ng
- **Parameter Sharing**: Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cáº§n há»c
- **Hierarchical Learning**: Há»c Ä‘áº·c trÆ°ng tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n phá»©c táº¡p
- **Spatial Hierarchy**: Giá»¯ nguyÃªn thÃ´ng tin khÃ´ng gian
- **Feature Detection**: Tá»± Ä‘á»™ng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng quan trá»ng
- **Robust**: Ãt bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi noise vÃ  biáº¿n dáº¡ng nhá»

## âš ï¸ Háº¡n cháº¿

- **Computation Intensive**: Tá»‘n nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n
- **Memory Requirements**: Cáº§n nhiá»u bá»™ nhá»› cho training
- **Data Hungry**: YÃªu cáº§u lÆ°á»£ng dá»¯ liá»‡u lá»›n
- **Hyperparameter Sensitive**: Nhiá»u tham sá»‘ cáº§n Ä‘iá»u chá»‰nh
- **Limited to Grid-like Data**: Chá»§ yáº¿u cho dá»¯ liá»‡u dáº¡ng lÆ°á»›i
- **Black Box**: KhÃ³ giáº£i thÃ­ch quyáº¿t Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh

## ğŸ¯ á»¨ng dá»¥ng

### Computer Vision

- **Image Classification**: PhÃ¢n loáº¡i hÃ¬nh áº£nh (ImageNet, CIFAR)
- **Object Detection**: PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng (YOLO, R-CNN)
- **Image Segmentation**: PhÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh (U-Net, Mask R-CNN)
- **Face Recognition**: Nháº­n dáº¡ng khuÃ´n máº·t
- **Medical Imaging**: Cháº©n Ä‘oÃ¡n y khoa tá»« X-ray, MRI

### CÃ¡c lÄ©nh vá»±c khÃ¡c

- **Natural Language Processing**: Text classification, sentiment analysis
- **Time Series**: PhÃ¢n tÃ­ch dá»¯ liá»‡u chuá»—i thá»i gian
- **Speech Recognition**: Nháº­n dáº¡ng giá»ng nÃ³i
- **Game AI**: ChÆ¡i game (AlphaGo)
- **Autonomous Vehicles**: Xe tá»± lÃ¡i

## ğŸ› ï¸ Hyperparameters quan trá»ng

### Convolutional Layer

- **Number of filters**: Sá»‘ lÆ°á»£ng kernel
- **Kernel size**: KÃ­ch thÆ°á»›c bá»™ lá»c (3Ã—3, 5Ã—5, 7Ã—7)
- **Stride**: BÆ°á»›c nháº£y cá»§a kernel
- **Padding**: Same, Valid
- **Activation**: ReLU, Sigmoid, Tanh

### Pooling Layer

- **Pool size**: KÃ­ch thÆ°á»›c pooling window
- **Stride**: BÆ°á»›c nháº£y pooling
- **Type**: Max, Average, Global

### Training

- **Learning rate**: 0.001, 0.01, 0.1
- **Batch size**: 16, 32, 64, 128
- **Optimizer**: Adam, SGD, RMSprop
- **Regularization**: Dropout, L1/L2, Batch Norm

## ğŸ’¡ Tips tá»‘i Æ°u

### Data Preparation

- **Data Augmentation**: Rotation, flip, crop, zoom
- **Normalization**: Pixel values to [0,1] or [-1,1]
- **Preprocessing**: Resize, center crop

### Architecture Design

- **Start Small**: Báº¯t Ä‘áº§u vá»›i máº¡ng Ä‘Æ¡n giáº£n
- **Progressive Growing**: TÄƒng dáº§n Ä‘á»™ phá»©c táº¡p
- **Skip Connections**: Sá»­ dá»¥ng residual connections
- **Batch Normalization**: á»”n Ä‘á»‹nh training

### Training Strategy

- **Transfer Learning**: Sá»­ dá»¥ng pre-trained models
- **Learning Rate Scheduling**: Giáº£m dáº§n learning rate
- **Early Stopping**: TrÃ¡nh overfitting
- **Ensemble Methods**: Káº¿t há»£p nhiá»u models

## ğŸ“ˆ Metrics Ä‘Ã¡nh giÃ¡

### Classification

- **Accuracy**: Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ
- **Precision**: Äá»™ chÃ­nh xÃ¡c dÆ°Æ¡ng tÃ­nh
- **Recall**: Äá»™ nháº¡y
- **F1-Score**: Äiá»ƒm F1
- **Confusion Matrix**: Ma tráº­n nháº§m láº«n

### Object Detection

- **mAP**: Mean Average Precision
- **IoU**: Intersection over Union
- **Precision-Recall Curve**: ÄÆ°á»ng cong P-R

## ğŸ”„ So sÃ¡nh vá»›i MLP

| Äáº·c Ä‘iá»ƒm               | MLP          | CNN              |
| ---------------------- | ------------ | ---------------- |
| Input                  | Vector 1D    | Ma tráº­n 2D/3D    |
| Parameters             | Nhiá»u        | Ãt hÆ¡n (sharing) |
| Spatial Info           | Máº¥t          | Giá»¯ nguyÃªn       |
| Translation Invariance | KhÃ´ng        | CÃ³               |
| Tá»‘c Ä‘á»™ training        | Nhanh        | Cháº­m hÆ¡n         |
| á»¨ng dá»¥ng chÃ­nh         | Tabular data | Image data       |

## ğŸš€ Kiáº¿n trÃºc hiá»‡n Ä‘áº¡i

### Vision Transformer (ViT)

- Ãp dá»¥ng Transformer cho computer vision
- Cáº¡nh tranh vá»›i CNN trong má»™t sá»‘ tÃ¡c vá»¥

### EfficientNet

- Tá»‘i Æ°u hÃ³a depth, width, resolution
- Hiá»‡u quáº£ tÃ­nh toÃ¡n cao

### MobileNet

- Thiáº¿t káº¿ cho mobile devices
- Depthwise separable convolutions

## ğŸ’¡ Káº¿t luáº­n

CNN Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a lÄ©nh vá»±c computer vision vÃ  trá»Ÿ thÃ nh xÆ°Æ¡ng sá»‘ng cá»§a nhiá»u á»©ng dá»¥ng AI hiá»‡n Ä‘áº¡i. Tá»« nhá»¯ng á»©ng dá»¥ng Ä‘Æ¡n giáº£n nhÆ° phÃ¢n loáº¡i hÃ¬nh áº£nh Ä‘áº¿n cÃ¡c há»‡ thá»‘ng phá»©c táº¡p nhÆ° xe tá»± lÃ¡i, CNN Ä‘Ã£ chá»©ng minh sá»©c máº¡nh vÆ°á»£t trá»™i trong viá»‡c xá»­ lÃ½ dá»¯ liá»‡u hÃ¬nh áº£nh. Hiá»ƒu rÃµ CNN khÃ´ng chá»‰ giÃºp phÃ¡t triá»ƒn cÃ¡c á»©ng dá»¥ng computer vision mÃ  cÃ²n lÃ  ná»n táº£ng Ä‘á»ƒ tiáº¿p cáº­n cÃ¡c kiáº¿n trÃºc tiÃªn tiáº¿n khÃ¡c nhÆ° Transformer vÃ  cÃ¡c mÃ´ hÃ¬nh multimodal.
