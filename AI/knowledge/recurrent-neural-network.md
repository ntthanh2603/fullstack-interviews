# ğŸ”„ Recurrent Neural Networks: RNN, LSTM & GRU

## ğŸ“– Giá»›i thiá»‡u

Recurrent Neural Networks (RNNs) lÃ  má»™t há» máº¡ng neural Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± (sequential data). KhÃ¡c vá»›i feedforward networks, RNNs cÃ³ kháº£ nÄƒng "nhá»›" thÃ´ng tin tá»« cÃ¡c bÆ°á»›c thá»i gian trÆ°á»›c Ä‘Ã³ thÃ´ng qua cÃ¡c káº¿t ná»‘i láº·p láº¡i (recurrent connections). Äiá»u nÃ y lÃ m cho RNNs trá»Ÿ thÃ nh lá»±a chá»n lÃ½ tÆ°á»Ÿng cho cÃ¡c tÃ¡c vá»¥ liÃªn quan Ä‘áº¿n chuá»—i dá»¯ liá»‡u nhÆ° xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, nháº­n dáº¡ng giá»ng nÃ³i, vÃ  dá»± Ä‘oÃ¡n chuá»—i thá»i gian.

![RNN Family](../assets/RNN.jpg)

---

## ğŸ¯ RNN (Recurrent Neural Network)

### ğŸ—ï¸ Kiáº¿n trÃºc cÆ¡ báº£n

RNN cÆ¡ báº£n cÃ³ cáº¥u trÃºc Ä‘Æ¡n giáº£n vá»›i má»™t hidden state Ä‘Æ°á»£c truyá»n qua cÃ¡c time steps:

```
h_t = tanh(W_h Ã— h_{t-1} + W_x Ã— x_t + b)
y_t = W_y Ã— h_t + b_y
```

### âš™ï¸ CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng

1. **Input**: Nháº­n input sequence xâ‚, xâ‚‚, ..., xâ‚œ
2. **Hidden State**: Duy trÃ¬ hidden state h_t qua cÃ¡c time steps
3. **Memory**: h_t chá»©a thÃ´ng tin tá»« táº¥t cáº£ inputs trÆ°á»›c Ä‘Ã³
4. **Output**: Táº¡o output y_t táº¡i má»—i time step (tÃ¹y chá»n)

### ğŸ”„ CÃ¡c kiá»ƒu RNN

- **One-to-One**: Input vÃ  output Ä‘Æ¡n láº»
- **One-to-Many**: Má»™t input, nhiá»u outputs (text generation)
- **Many-to-One**: Nhiá»u inputs, má»™t output (sentiment analysis)
- **Many-to-Many**: Nhiá»u inputs vÃ  outputs (machine translation)

### âœ… Æ¯u Ä‘iá»ƒm RNN

- **Sequential Processing**: Xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± tá»± nhiÃªn
- **Memory**: CÃ³ kháº£ nÄƒng ghi nhá»› thÃ´ng tin tá»« quÃ¡ khá»©
- **Variable Length**: Xá»­ lÃ½ sequences cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau
- **Parameter Sharing**: Chia sáº» tham sá»‘ qua time steps
- **Flexibility**: Nhiá»u kiáº¿n trÃºc input-output khÃ¡c nhau

### âš ï¸ Háº¡n cháº¿ RNN

- **Vanishing Gradient**: Gradient biáº¿n máº¥t á»Ÿ sequences dÃ i
- **Short-term Memory**: KhÃ³ ghi nhá»› thÃ´ng tin xa trong quÃ¡ khá»©
- **Sequential Processing**: KhÃ´ng thá»ƒ song song hÃ³a training
- **Exploding Gradient**: Gradient cÃ³ thá»ƒ tÄƒng Ä‘á»™t ngá»™t
- **Computational Inefficiency**: Cháº­m vá»›i sequences dÃ i

---

![LSTM](../assets/LSTM-GRU.jpg)

## ğŸšª LSTM (Long Short-Term Memory)

### ğŸ“– Giá»›i thiá»‡u LSTM

LSTM Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Hochreiter vÃ  Schmidhuber (1997) Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient cá»§a RNN. LSTM sá»­ dá»¥ng má»™t cÆ¡ cháº¿ gate phá»©c táº¡p Ä‘á»ƒ kiá»ƒm soÃ¡t luá»“ng thÃ´ng tin.

### ğŸ—ï¸ Kiáº¿n trÃºc LSTM

LSTM cÃ³ 4 thÃ nh pháº§n chÃ­nh:

1. **Cell State (C_t)**: DÃ²ng thÃ´ng tin chÃ­nh
2. **Forget Gate**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n quÃªn
3. **Input Gate**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin má»›i nÃ o cáº§n lÆ°u
4. **Output Gate**: Quyáº¿t Ä‘á»‹nh pháº§n nÃ o cá»§a cell state sáº½ output

### ğŸ§® CÃ´ng thá»©c LSTM

```
# Forget Gate
f_t = Ïƒ(W_f Ã— [h_{t-1}, x_t] + b_f)

# Input Gate
i_t = Ïƒ(W_i Ã— [h_{t-1}, x_t] + b_i)
CÌƒ_t = tanh(W_C Ã— [h_{t-1}, x_t] + b_C)

# Cell State Update
C_t = f_t Ã— C_{t-1} + i_t Ã— CÌƒ_t

# Output Gate
o_t = Ïƒ(W_o Ã— [h_{t-1}, x_t] + b_o)
h_t = o_t Ã— tanh(C_t)
```

### âš™ï¸ CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng LSTM

1. **Forget Gate**: XÃ³a thÃ´ng tin khÃ´ng cáº§n thiáº¿t tá»« cell state
2. **Input Gate**: Chá»n thÃ´ng tin má»›i Ä‘á»ƒ lÆ°u vÃ o cell state
3. **Update Cell State**: Cáº­p nháº­t cell state vá»›i thÃ´ng tin má»›i
4. **Output Gate**: Táº¡o hidden state tá»« cell state

### âœ… Æ¯u Ä‘iá»ƒm LSTM

- **Long-term Memory**: Ghi nhá»› thÃ´ng tin trong thá»i gian dÃ i
- **Gradient Flow**: Giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient
- **Selective Memory**: Chá»n lá»c thÃ´ng tin cáº§n ghi nhá»›/quÃªn
- **Stable Training**: Training á»•n Ä‘á»‹nh hÆ¡n RNN cÆ¡ báº£n
- **Versatile**: Hiá»‡u quáº£ cho nhiá»u tÃ¡c vá»¥ sequence

### âš ï¸ Háº¡n cháº¿ LSTM

- **Computational Complexity**: Phá»©c táº¡p hÆ¡n RNN cÆ¡ báº£n
- **Memory Requirements**: Cáº§n nhiá»u bá»™ nhá»› hÆ¡n
- **Training Time**: LÃ¢u hÆ¡n Ä‘á»ƒ training
- **Overfitting**: Dá»… overfit vá»›i datasets nhá»

---

## âš¡ GRU (Gated Recurrent Unit)

### ğŸ“– Giá»›i thiá»‡u GRU

GRU Ä‘Æ°á»£c giá»›i thiá»‡u bá»Ÿi Cho et al. (2014) nhÆ° má»™t phiÃªn báº£n Ä‘Æ¡n giáº£n hÃ³a cá»§a LSTM. GRU káº¿t há»£p forget gate vÃ  input gate thÃ nh má»™t update gate, lÃ m giáº£m Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n.

### ğŸ—ï¸ Kiáº¿n trÃºc GRU

GRU cÃ³ 2 gates chÃ­nh:

1. **Reset Gate (r_t)**: Quyáº¿t Ä‘á»‹nh bá» quÃªn bao nhiá»u thÃ´ng tin tá»« quÃ¡ khá»©
2. **Update Gate (z_t)**: Quyáº¿t Ä‘á»‹nh cáº­p nháº­t bao nhiá»u thÃ´ng tin má»›i

### ğŸ§® CÃ´ng thá»©c GRU

```
# Reset Gate
r_t = Ïƒ(W_r Ã— [h_{t-1}, x_t] + b_r)

# Update Gate
z_t = Ïƒ(W_z Ã— [h_{t-1}, x_t] + b_z)

# Candidate Hidden State
hÌƒ_t = tanh(W_h Ã— [r_t Ã— h_{t-1}, x_t] + b_h)

# Final Hidden State
h_t = (1 - z_t) Ã— h_{t-1} + z_t Ã— hÌƒ_t
```

### âš™ï¸ CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng GRU

1. **Reset Gate**: Kiá»ƒm soÃ¡t viá»‡c káº¿t há»£p thÃ´ng tin tá»« quÃ¡ khá»©
2. **Update Gate**: CÃ¢n báº±ng giá»¯a thÃ´ng tin cÅ© vÃ  má»›i
3. **Candidate State**: Táº¡o hidden state á»©ng viÃªn
4. **Final State**: Káº¿t há»£p thÃ´ng tin cÅ© vÃ  má»›i theo tá»· lá»‡

### âœ… Æ¯u Ä‘iá»ƒm GRU

- **Simpler Architecture**: ÄÆ¡n giáº£n hÆ¡n LSTM (2 gates vs 3 gates)
- **Faster Training**: Training nhanh hÆ¡n LSTM
- **Fewer Parameters**: Ãt tham sá»‘ hÆ¡n LSTM
- **Good Performance**: Hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng LSTM trong nhiá»u tÃ¡c vá»¥
- **Less Overfitting**: Ãt overfit hÆ¡n do Ã­t tham sá»‘

### âš ï¸ Háº¡n cháº¿ GRU

- **Less Control**: Ãt kiá»ƒm soÃ¡t hÆ¡n LSTM Ä‘á»‘i vá»›i luá»“ng thÃ´ng tin
- **Recent Development**: Ãt nghiÃªn cá»©u hÆ¡n LSTM
- **Task Dependent**: Hiá»‡u suáº¥t phá»¥ thuá»™c vÃ o tÃ¡c vá»¥ cá»¥ thá»ƒ

---

## ğŸ”„ So sÃ¡nh RNN, LSTM, GRU

| Äáº·c Ä‘iá»ƒm             | RNN               | LSTM              | GRU                       |
| -------------------- | ----------------- | ----------------- | ------------------------- |
| **Äá»™ phá»©c táº¡p**      | Tháº¥p              | Cao               | Trung bÃ¬nh                |
| **Sá»‘ parameters**    | Ãt                | Nhiá»u             | Trung bÃ¬nh                |
| **Tá»‘c Ä‘á»™ training**  | Nhanh             | Cháº­m              | Trung bÃ¬nh                |
| **Memory capacity**  | Ngáº¯n              | DÃ i               | DÃ i                       |
| **Gradient problem** | CÃ³                | Ãt                | Ãt                        |
| **Hiá»‡u suáº¥t**        | KÃ©m               | Tá»‘t               | Tá»‘t                       |
| **Use case**         | Sequence Ä‘Æ¡n giáº£n | Sequence phá»©c táº¡p | CÃ¢n báº±ng hiá»‡u suáº¥t/tá»‘c Ä‘á»™ |

## ğŸ¯ á»¨ng dá»¥ng

### Natural Language Processing

- **Machine Translation**: Dá»‹ch mÃ¡y (Seq2Seq)
- **Text Generation**: Táº¡o vÄƒn báº£n, thÆ¡, code
- **Sentiment Analysis**: PhÃ¢n tÃ­ch cáº£m xÃºc
- **Named Entity Recognition**: Nháº­n dáº¡ng thá»±c thá»ƒ
- **Chatbots**: Há»‡ thá»‘ng há»i Ä‘Ã¡p tá»± Ä‘á»™ng

### Speech & Audio

- **Speech Recognition**: Nháº­n dáº¡ng giá»ng nÃ³i
- **Speech Synthesis**: Tá»•ng há»£p giá»ng nÃ³i
- **Music Generation**: Táº¡o nháº¡c tá»± Ä‘á»™ng
- **Audio Classification**: PhÃ¢n loáº¡i Ã¢m thanh

### Time Series

- **Stock Prediction**: Dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u
- **Weather Forecasting**: Dá»± bÃ¡o thá»i tiáº¿t
- **Sales Forecasting**: Dá»± Ä‘oÃ¡n doanh sá»‘
- **Anomaly Detection**: PhÃ¡t hiá»‡n báº¥t thÆ°á»ng

### Computer Vision

- **Video Analysis**: PhÃ¢n tÃ­ch video
- **Action Recognition**: Nháº­n dáº¡ng hÃ nh Ä‘á»™ng
- **Image Captioning**: Táº¡o chÃº thÃ­ch hÃ¬nh áº£nh
- **Object Tracking**: Theo dÃµi Ä‘á»‘i tÆ°á»£ng

## ğŸ› ï¸ Hyperparameters quan trá»ng

### Architecture

- **Hidden Size**: Sá»‘ neurons trong hidden layer
- **Number of Layers**: Sá»‘ lá»›p RNN (1-3 layers thÆ°á»ng)
- **Bidirectional**: RNN hai chiá»u
- **Dropout**: Tá»· lá»‡ dropout giá»¯a cÃ¡c layers

### Training

- **Learning Rate**: 0.001, 0.01 (thÆ°á»ng nhá» hÆ¡n CNN)
- **Batch Size**: 32, 64, 128
- **Sequence Length**: Äá»™ dÃ i sequence input
- **Gradient Clipping**: NgÄƒn exploding gradient

### Optimization

- **Optimizer**: Adam, RMSprop, SGD
- **Loss Function**: CrossEntropy, MSE, CTC
- **Regularization**: L1/L2, Dropout, Batch Norm

## ğŸ’¡ Tips tá»‘i Æ°u

### Data Preparation

- **Sequence Padding**: Äá»“ng nháº¥t Ä‘á»™ dÃ i sequences
- **Normalization**: Chuáº©n hÃ³a dá»¯ liá»‡u Ä‘áº§u vÃ o
- **Tokenization**: Chuyá»ƒn text thÃ nh tokens
- **Bucketing**: NhÃ³m sequences theo Ä‘á»™ dÃ i

### Training Strategy

- **Gradient Clipping**: Clip gradients Ä‘á»ƒ trÃ¡nh exploding
- **Teacher Forcing**: Sá»­ dá»¥ng ground truth lÃ m input
- **Curriculum Learning**: Training tá»« dá»… Ä‘áº¿n khÃ³
- **Attention Mechanism**: ThÃªm attention Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t

### Architecture Choices

- **LSTM vs GRU**: GRU cho tá»‘c Ä‘á»™, LSTM cho hiá»‡u suáº¥t
- **Bidirectional**: Sá»­ dá»¥ng khi cÃ³ thá»ƒ nhÃ¬n cáº£ quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai
- **Stacking**: 2-3 layers thÆ°á»ng tá»‘i Æ°u
- **Residual Connections**: Cho máº¡ng sÃ¢u

## ğŸ“ˆ Metrics Ä‘Ã¡nh giÃ¡

### Text Generation

- **Perplexity**: Äá»™ bá»‘i rá»‘i cá»§a model
- **BLEU Score**: Cháº¥t lÆ°á»£ng translation
- **ROUGE Score**: Cháº¥t lÆ°á»£ng summarization

### Classification

- **Accuracy**: Äá»™ chÃ­nh xÃ¡c
- **F1-Score**: CÃ¢n báº±ng precision-recall
- **Confusion Matrix**: Ma tráº­n nháº§m láº«n

### Time Series

- **MAE**: Mean Absolute Error
- **RMSE**: Root Mean Square Error
- **MAPE**: Mean Absolute Percentage Error

## ğŸš€ Xu hÆ°á»›ng hiá»‡n Ä‘áº¡i

### Transformer Architecture

- **Self-Attention**: Thay tháº¿ RNN trong nhiá»u tÃ¡c vá»¥
- **BERT, GPT**: Pre-trained language models
- **Parallel Processing**: Xá»­ lÃ½ song song hiá»‡u quáº£

### Hybrid Approaches

- **CNN-RNN**: Káº¿t há»£p CNN vÃ  RNN
- **Attention + RNN**: ThÃªm attention mechanism
- **Graph RNN**: RNN cho dá»¯ liá»‡u Ä‘á»“ thá»‹

## ğŸ’¡ Káº¿t luáº­n

RNN, LSTM, vÃ  GRU táº¡o thÃ nh bá»™ ba cÃ´ng cá»¥ máº¡nh máº½ cho viá»‡c xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»±. Má»—i kiáº¿n trÃºc cÃ³ nhá»¯ng Æ°u nhÆ°á»£c Ä‘iá»ƒm riÃªng:

- **RNN**: ÄÆ¡n giáº£n, nhanh, phÃ¹ há»£p cho sequences ngáº¯n
- **LSTM**: Máº¡nh máº½, nhá»› lÃ¢u, phÃ¹ há»£p cho tÃ¡c vá»¥ phá»©c táº¡p
- **GRU**: CÃ¢n báº±ng giá»¯a hiá»‡u suáº¥t vÃ  tá»‘c Ä‘á»™

Máº·c dÃ¹ Transformer Ä‘Ã£ thay tháº¿ RNNs trong nhiá»u tÃ¡c vá»¥ NLP, hiá»ƒu rÃµ RNN family váº«n quan trá»ng vÃ¬ chÃºng váº«n há»¯u Ã­ch trong nhiá»u á»©ng dá»¥ng Ä‘áº·c biá»‡t vÃ  lÃ  ná»n táº£ng Ä‘á»ƒ hiá»ƒu cÃ¡c kiáº¿n trÃºc sequence modeling hiá»‡n Ä‘áº¡i.
