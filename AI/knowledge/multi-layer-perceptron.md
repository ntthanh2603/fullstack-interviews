# ğŸ”— Multi Layer Perceptron (MLP)

## ğŸ“– Giá»›i thiá»‡u

Multi Layer Perceptron (MLP) lÃ  má»™t máº¡ng neural nhÃ¢n táº¡o gá»“m nhiá»u lá»›p perceptron Ä‘Æ°á»£c káº¿t ná»‘i vá»›i nhau. MLP lÃ  sá»± má»Ÿ rá»™ng cá»§a Single Layer Perceptron, Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ kháº¯c phá»¥c nhá»¯ng háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh má»™t lá»›p vÃ  cÃ³ kháº£ nÄƒng giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n phi tuyáº¿n phá»©c táº¡p.

![MLP](../assets/mlp.png)

## ğŸ—ï¸ Kiáº¿n trÃºc

MLP bao gá»“m:

- **Lá»›p Ä‘áº§u vÃ o (Input Layer)**: Nháº­n dá»¯ liá»‡u Ä‘áº§u vÃ o
- **Lá»›p áº©n (Hidden Layer)**: Má»™t hoáº·c nhiá»u lá»›p xá»­ lÃ½ trung gian
- **Lá»›p Ä‘áº§u ra (Output Layer)**: Táº¡o ra káº¿t quáº£ cuá»‘i cÃ¹ng

Má»—i lá»›p Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ (fully connected) vá»›i lá»›p tiáº¿p theo.

## âš™ï¸ CÃ¡ch thá»©c hoáº¡t Ä‘á»™ng

### Forward Propagation

1. **Äáº§u vÃ o**: Dá»¯ liá»‡u Ä‘Æ°á»£c truyá»n tá»« lá»›p input
2. **TÃ­nh toÃ¡n lá»›p áº©n**: Má»—i neuron tÃ­nh `z = Î£(wáµ¢ Ã— xáµ¢) + b`
3. **HÃ m kÃ­ch hoáº¡t**: Ãp dá»¥ng hÃ m kÃ­ch hoáº¡t `a = f(z)`
4. **Truyá»n tiáº¿p**: Káº¿t quáº£ Ä‘Æ°á»£c truyá»n Ä‘áº¿n lá»›p tiáº¿p theo
5. **Äáº§u ra**: Lá»›p cuá»‘i táº¡o ra dá»± Ä‘oÃ¡n

### Backpropagation

1. **TÃ­nh sai sá»‘**: So sÃ¡nh Ä‘áº§u ra vá»›i giÃ¡ trá»‹ thá»±c
2. **Lan truyá»n ngÆ°á»£c**: Truyá»n sai sá»‘ ngÆ°á»£c vá» cÃ¡c lá»›p trÆ°á»›c
3. **Cáº­p nháº­t trá»ng sá»‘**: Sá»­ dá»¥ng gradient descent Ä‘á»ƒ Ä‘iá»u chá»‰nh weights vÃ  bias

## ğŸ§® CÃ´ng thá»©c

### Forward Pass

```
zâ±¼â½Ë¡â¾ = Î£áµ¢ wáµ¢â±¼â½Ë¡â¾ Ã— aáµ¢â½Ë¡â»Â¹â¾ + bâ±¼â½Ë¡â¾
aâ±¼â½Ë¡â¾ = f(zâ±¼â½Ë¡â¾)
```

### Backpropagation

```
Î´â±¼â½Ë¡â¾ = (âˆ‚C/âˆ‚zâ±¼â½Ë¡â¾)
âˆ‚C/âˆ‚wáµ¢â±¼â½Ë¡â¾ = aáµ¢â½Ë¡â»Â¹â¾ Ã— Î´â±¼â½Ë¡â¾
```

Trong Ä‘Ã³:

- `l`: chá»‰ sá»‘ lá»›p
- `w`: trá»ng sá»‘
- `a`: giÃ¡ trá»‹ kÃ­ch hoáº¡t
- `z`: tá»•ng cÃ³ trá»ng sá»‘
- `Î´`: sai sá»‘
- `C`: hÃ m cost

## ğŸ“Š HÃ m kÃ­ch hoáº¡t

- **Sigmoid**: `Ïƒ(x) = 1/(1 + e^(-x))`
- **ReLU**: `f(x) = max(0, x)`
- **Tanh**: `tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))`
- **Softmax**: `softmax(xáµ¢) = e^(xáµ¢)/Î£â±¼e^(xâ±¼)`

## âœ… Æ¯u Ä‘iá»ƒm

- **Kháº£ nÄƒng phi tuyáº¿n**: Giáº£i quyáº¿t Ä‘Æ°á»£c cÃ¡c bÃ i toÃ¡n phi tuyáº¿n phá»©c táº¡p
- **Linh hoáº¡t**: CÃ³ thá»ƒ Ä‘iá»u chá»‰nh sá»‘ lá»›p vÃ  sá»‘ neuron
- **Universal Approximator**: CÃ³ thá»ƒ xáº¥p xá»‰ báº¥t ká»³ hÃ m liÃªn tá»¥c nÃ o
- **Äa dáº¡ng á»©ng dá»¥ng**: PhÃ¹ há»£p vá»›i nhiá»u loáº¡i bÃ i toÃ¡n
- **Kháº£ nÄƒng há»c**: Tá»± Ä‘á»™ng há»c cÃ¡c Ä‘áº·c trÆ°ng tá»« dá»¯ liá»‡u

## âš ï¸ Háº¡n cháº¿

- **Phá»©c táº¡p**: KhÃ³ Ä‘iá»u chá»‰nh vÃ  tá»‘i Æ°u hÃ³a
- **Overfitting**: Dá»… bá»‹ há»c thuá»™c lÃ²ng dá»¯ liá»‡u training
- **Vanishing Gradient**: Gradient cÃ³ thá»ƒ biáº¿n máº¥t á»Ÿ cÃ¡c lá»›p sÃ¢u
- **TÃ­nh toÃ¡n**: YÃªu cáº§u nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n
- **Black Box**: KhÃ³ giáº£i thÃ­ch cÃ¡ch mÃ´ hÃ¬nh Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh

## ğŸ¯ á»¨ng dá»¥ng

- **PhÃ¢n loáº¡i hÃ¬nh áº£nh**: Nháº­n dáº¡ng Ä‘á»‘i tÆ°á»£ng, khuÃ´n máº·t
- **Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn**: PhÃ¢n tÃ­ch sentiment, dá»‹ch mÃ¡y
- **Dá»± Ä‘oÃ¡n**: GiÃ¡ cá»• phiáº¿u, thá»i tiáº¿t, xu hÆ°á»›ng thá»‹ trÆ°á»ng
- **Y táº¿**: Cháº©n Ä‘oÃ¡n bá»‡nh, phÃ¢n tÃ­ch hÃ¬nh áº£nh y khoa
- **TÃ i chÃ­nh**: PhÃ¡t hiá»‡n gian láº­n, Ä‘Ã¡nh giÃ¡ rá»§i ro
- **Game AI**: Cá» vua, Go, game strategy

## ğŸ› ï¸ Hyperparameters quan trá»ng

- **Sá»‘ lá»›p áº©n**: Äá»™ sÃ¢u cá»§a máº¡ng
- **Sá»‘ neuron má»—i lá»›p**: Äá»™ rá»™ng cá»§a máº¡ng
- **Learning rate**: Tá»‘c Ä‘á»™ há»c
- **Batch size**: KÃ­ch thÆ°á»›c máº» dá»¯ liá»‡u
- **Epochs**: Sá»‘ láº§n huáº¥n luyá»‡n
- **Regularization**: L1, L2, Dropout

## ğŸ’¡ Tips tá»‘i Æ°u

- **Chuáº©n hÃ³a dá»¯ liá»‡u**: Normalize input data
- **Khá»Ÿi táº¡o trá»ng sá»‘**: Sá»­ dá»¥ng Xavier/He initialization
- **Regularization**: Ãp dá»¥ng dropout, batch normalization
- **Learning rate scheduling**: Giáº£m dáº§n learning rate
- **Early stopping**: Dá»«ng khi validation loss khÃ´ng cáº£i thiá»‡n
- **Cross-validation**: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh má»™t cÃ¡ch khÃ¡ch quan

## ğŸ”„ So sÃ¡nh vá»›i SLP

| Äáº·c Ä‘iá»ƒm           | SLP        | MLP       |
| ------------------ | ---------- | --------- |
| Sá»‘ lá»›p             | 1          | â‰¥ 2       |
| BÃ i toÃ¡n           | Tuyáº¿n tÃ­nh | Phi tuyáº¿n |
| Äá»™ phá»©c táº¡p        | Tháº¥p       | Cao       |
| Kháº£ nÄƒng há»c       | Háº¡n cháº¿    | Máº¡nh máº½   |
| Thá»i gian training | Nhanh      | Cháº­m      |

## ğŸ’¡ Káº¿t luáº­n

Multi Layer Perceptron lÃ  ná»n táº£ng quan trá»ng cá»§a deep learning hiá»‡n Ä‘áº¡i. Vá»›i kháº£ nÄƒng há»c cÃ¡c máº«u phá»©c táº¡p vÃ  Ä‘a dáº¡ng á»©ng dá»¥ng, MLP Ä‘Ã£ má»Ÿ Ä‘Æ°á»ng cho sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c kiáº¿n trÃºc neural network tiÃªn tiáº¿n nhÆ° CNN, RNN, vÃ  Transformer. Viá»‡c hiá»ƒu rÃµ MLP lÃ  bÆ°á»›c Ä‘á»‡m quan trá»ng Ä‘á»ƒ tiáº¿p cáº­n cÃ¡c cÃ´ng nghá»‡ AI phá»©c táº¡p hÆ¡n.
