# ğŸ”„ Transformer Architecture

## ğŸ“– Giá»›i thiá»‡u

Transformer lÃ  kiáº¿n trÃºc máº¡ng neural cÃ¡ch máº¡ng Ä‘Æ°á»£c giá»›i thiá»‡u trong paper "Attention Is All You Need" (2017) bá»Ÿi team Google. KhÃ¡c vá»›i RNN/LSTM xá»­ lÃ½ tuáº§n tá»±, Transformer sá»­ dá»¥ng hoÃ n toÃ n cÆ¡ cháº¿ Self-Attention Ä‘á»ƒ xá»­ lÃ½ song song, táº¡o ra bÆ°á»›c ngoáº·t trong NLP vÃ  trá»Ÿ thÃ nh ná»n táº£ng cho GPT, BERT, T5.

![Transformer](../assets/transformer.png)

## ğŸ—ï¸ Kiáº¿n trÃºc tá»•ng thá»ƒ

Transformer gá»“m hai pháº§n chÃ­nh:

### Encoder Stack (6 layers)

- **Multi-Head Self-Attention**
- **Position-wise Feed-Forward Network**
- **Residual Connections & Layer Normalization**

### Decoder Stack (6 layers)

- **Masked Multi-Head Self-Attention**
- **Multi-Head Cross-Attention** (Encoder-Decoder)
- **Position-wise Feed-Forward Network**
- **Residual Connections & Layer Normalization**

---

## ğŸ”§ CÃ¡c thÃ nh pháº§n chÃ­nh

### 1. ğŸ¯ Self-Attention Mechanism

**Má»¥c Ä‘Ã­ch**: Cho phÃ©p model "chÃº Ã½" Ä‘áº¿n cÃ¡c pháº§n khÃ¡c nhau cá»§a input sequence

**CÃ´ng thá»©c**:

```
Attention(Q,K,V) = softmax(QK^T / âˆšd_k) Ã— V
```

**CÃ¡ch hoáº¡t Ä‘á»™ng**:

1. **Query (Q)**: "TÃ´i Ä‘ang tÃ¬m kiáº¿m thÃ´ng tin gÃ¬?"
2. **Key (K)**: "TÃ´i chá»©a thÃ´ng tin nÃ o?"
3. **Value (V)**: "ThÃ´ng tin thá»±c táº¿ lÃ  gÃ¬?"
4. **Scores**: TÃ­nh similarity giá»¯a Q vÃ  táº¥t cáº£ K
5. **Weights**: Softmax Ä‘á»ƒ táº¡o attention weights
6. **Output**: Weighted sum cá»§a táº¥t cáº£ V

### 2. ğŸ”€ Multi-Head Attention

**Má»¥c Ä‘Ã­ch**: Há»c nhiá»u loáº¡i quan há»‡ khÃ¡c nhau cÃ¹ng lÃºc

```
MultiHead(Q,K,V) = Concat(headâ‚, headâ‚‚, ..., head_h) Ã— W^O

head_i = Attention(QÃ—W_i^Q, KÃ—W_i^K, VÃ—W_i^V)
```

**Æ¯u Ä‘iá»ƒm**:

- Há»c Ä‘Æ°á»£c nhiá»u patterns Ä‘á»“ng thá»i
- TÄƒng kháº£ nÄƒng biá»ƒu diá»…n cá»§a model
- Má»—i head táº­p trung vÃ o aspect khÃ¡c nhau

### 3. ğŸ“ Positional Encoding

**Má»¥c Ä‘Ã­ch**: ThÃªm thÃ´ng tin vá»‹ trÃ­ vÃ o embeddings

```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**Táº¡i sao cáº§n**:

- Attention khÃ´ng cÃ³ khÃ¡i niá»‡m thá»© tá»±
- GiÃºp model hiá»ƒu vá»‹ trÃ­ cá»§a tá»« trong cÃ¢u

### 4. ğŸ”„ Feed-Forward Network

**Kiáº¿n trÃºc**:

```
FFN(x) = max(0, xÃ—Wâ‚ + bâ‚)Ã—Wâ‚‚ + bâ‚‚
```

**Äáº·c Ä‘iá»ƒm**:

- 2 linear layers vá»›i ReLU á»Ÿ giá»¯a
- Hidden dimension thÆ°á»ng 4x model dimension
- Ãp dá»¥ng Ä‘á»™c láº­p cho má»—i position

### 5. ğŸ”— Residual Connections & Layer Norm

**Residual**: `LayerNorm(x + Sublayer(x))`

**Lá»£i Ã­ch**:

- Giáº£i quyáº¿t vanishing gradient problem
- GiÃºp training cÃ¡c model sÃ¢u
- Layer Norm á»•n Ä‘á»‹nh training

### 6. ğŸ­ Masked Attention (Decoder)

**Má»¥c Ä‘Ã­ch**: NgÄƒn model nhÃ¬n tháº¥y future tokens

**CÃ¡ch hoáº¡t Ä‘á»™ng**:

- Mask future positions vá»›i -âˆ
- Sau softmax â†’ attention weight = 0
- Äáº£m báº£o autoregressive generation

---

## âš™ï¸ CÃ¡ch hoáº¡t Ä‘á»™ng chi tiáº¿t

### ğŸ” Encoder Process

1. **Input Embedding**: Token â†’ Vector
2. **Positional Encoding**: ThÃªm thÃ´ng tin vá»‹ trÃ­
3. **Self-Attention**: Má»—i token "nhÃ¬n" táº¥t cáº£ tokens khÃ¡c
4. **Add & Norm**: Residual connection + LayerNorm
5. **Feed-Forward**: Xá»­ lÃ½ tá»«ng position Ä‘á»™c láº­p
6. **Add & Norm**: Residual connection + LayerNorm
7. **Repeat**: 6 encoder layers

### ğŸ¯ Decoder Process

1. **Output Embedding**: Previous tokens â†’ Vectors
2. **Positional Encoding**: ThÃªm thÃ´ng tin vá»‹ trÃ­
3. **Masked Self-Attention**: Chá»‰ nhÃ¬n previous tokens
4. **Add & Norm**: Residual + LayerNorm
5. **Cross-Attention**: Attend to encoder output
6. **Add & Norm**: Residual + LayerNorm
7. **Feed-Forward**: Xá»­ lÃ½ position-wise
8. **Add & Norm**: Residual + LayerNorm
9. **Linear + Softmax**: Dá»± Ä‘oÃ¡n next token

### ğŸ”„ Training Process

1. **Teacher Forcing**: Sá»­ dá»¥ng ground truth lÃ m decoder input
2. **Parallel Training**: Táº¥t cáº£ positions Ä‘Æ°á»£c tÃ­nh Ä‘á»“ng thá»i
3. **Loss Calculation**: CrossEntropy cho tá»«ng position
4. **Backpropagation**: Update táº¥t cáº£ parameters

---

## âœ… Æ¯u Ä‘iá»ƒm

- **Parallelization**: Xá»­ lÃ½ song song â†’ training nhanh
- **Long-range Dependencies**: Attention trá»±c tiáº¿p giá»¯a má»i cáº·p tokens
- **Scalability**: Scale tá»‘t vá»›i dá»¯ liá»‡u vÃ  compute lá»›n
- **Transfer Learning**: Pre-trained models hiá»‡u quáº£
- **Interpretability**: Attention weights cÃ³ thá»ƒ visualize

## âš ï¸ Háº¡n cháº¿

- **Quadratic Complexity**: O(nÂ²) vá»›i sequence length
- **Memory Requirements**: Cáº§n nhiá»u GPU memory
- **Data Hungry**: YÃªu cáº§u dataset lá»›n Ä‘á»ƒ training hiá»‡u quáº£
- **Computational Cost**: Expensive Ä‘á»ƒ train tá»« Ä‘áº§u

---

## ğŸ¯ á»¨ng dá»¥ng

### Language Models

- **GPT series**: Text generation, chat
- **BERT**: Understanding tasks, classification
- **T5**: Text-to-text transfer transformer

### Computer Vision

- **Vision Transformer (ViT)**: Image classification
- **DETR**: Object detection
- **Swin Transformer**: Hierarchical vision

### Multimodal

- **CLIP**: Vision-language understanding
- **DALL-E**: Text-to-image generation
- **Flamingo**: Few-shot learning

---

## ğŸ› ï¸ Hyperparameters quan trá»ng

### Architecture

- **Model Dimension (d_model)**: 512, 768, 1024
- **Number of Heads**: 8, 12, 16
- **Number of Layers**: 6, 12, 24
- **Feed-Forward Dimension**: 2048, 3072, 4096

### Training

- **Learning Rate**: Warmup + decay schedule
- **Batch Size**: Lá»›n (1000+ samples)
- **Dropout**: 0.1, 0.2
- **Label Smoothing**: 0.1

---

## ğŸ’¡ Biáº¿n thá»ƒ quan trá»ng

### GPT (Generative Pre-trained Transformer)

- **Decoder-only**: Chá»‰ sá»­ dá»¥ng decoder stack
- **Autoregressive**: Generate tá»« trÃ¡i sang pháº£i
- **Causal Attention**: Masked self-attention

### BERT (Bidirectional Encoder Representations)

- **Encoder-only**: Chá»‰ sá»­ dá»¥ng encoder stack
- **Bidirectional**: NhÃ¬n cáº£ hai hÆ°á»›ng
- **Masked Language Model**: Predict masked tokens

### T5 (Text-to-Text Transfer Transformer)

- **Full Transformer**: Cáº£ encoder vÃ  decoder
- **Text-to-text**: Má»i task Ä‘á»u lÃ  text generation

---

## ğŸš€ Cáº£i tiáº¿n hiá»‡n Ä‘áº¡i

### Efficiency Improvements

- **Linformer**: Linear attention complexity
- **Performer**: Fast attention via random features
- **Longformer**: Sparse attention patterns

### Scale Improvements

- **Switch Transformer**: Sparse expert models
- **PaLM**: 540B parameter model
- **GPT-4**: Multimodal capabilities

---

## ğŸ’¡ Káº¿t luáº­n

Transformer Ä‘Ã£ cÃ¡ch máº¡ng hÃ³a AI vá»›i kiáº¿n trÃºc "Attention Is All You Need". Tá»« NLP Ä‘áº¿n Computer Vision, tá»« GPT Ä‘áº¿n ChatGPT, Transformer lÃ  ná»n táº£ng cá»§a háº§u háº¿t breakthrough trong AI hiá»‡n Ä‘áº¡i. Hiá»ƒu rÃµ Transformer lÃ  chÃ¬a khÃ³a Ä‘á»ƒ náº¯m báº¯t vÃ  phÃ¡t triá»ƒn cÃ¡c á»©ng dá»¥ng AI tiÃªn tiáº¿n.
